# Deep Learning
In this project, I aim to implement DistilBERT for a 7-label classification task using a Kaggle dataset (https://www.kaggle.com/datasets/suchintikasarkar/sentiment-analysis-for-mental-health). The dataset pertains to mental health sentiment analysis, and I will experiment with different approaches to handling class distribution to improve model performance.

Specifically, I conduct two different experiments, each focusing on strategies to manage class imbalances:

1. Baseline Experiment: Training the model on the original dataset without modifications to class distribution.
2. Stratified Sampling: Ensuring that each class is proportionally represented in the training, preventing any one class from dominating the learning process.

In addition to experimenting with different class distribution strategies, I will also tune the hyperparameters to evaluate their impact on model performance.
